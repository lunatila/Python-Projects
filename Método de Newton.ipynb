{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1118dfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import autograd ’s automatic differentiator\n",
    "from autograd import grad\n",
    "from autograd import hessian\n",
    "\n",
    "# import NumPy library\n",
    "import numpy as np\n",
    "\n",
    "# Newton ’s method\n",
    "def newtons_method (g, max_its, w):\n",
    "\n",
    "    # compute gradient/ Hessian using autograd\n",
    "    gradient = grad(g)\n",
    "    hess = hessian(g)\n",
    "    \n",
    "    # set numerical stability parameter\n",
    "    epsilon = 10 **(-7)\n",
    "    if 'epsilon' in kwargs:\n",
    "        epsilon = kwargs['epsilon']\n",
    "    \n",
    "    # run the Newton ’s method loop\n",
    "    weight_history = [w] # container for weight history\n",
    "    cost_history = [g(w)] # container for cost function history\n",
    "    for k in range( max_its ):\n",
    "    \n",
    "        # evaluate the gradient and hessian\n",
    "        grad_eval = gradient (w)\n",
    "        hess_eval = hess(w)\n",
    "    \n",
    "        # reshape hessian to square matrix\n",
    "        hess_eval .shape = (int((np. size( hess_eval ))**(0.5)),int((np.\n",
    "                            size( hess_eval ))**(0.5)))\n",
    "    \n",
    "        # solve second -order system for weight update\n",
    "        A = hess_eval + epsilon* np. eye(w. size)\n",
    "        b = grad_eval\n",
    "        w = np. linalg. solve(A, np .dot(A ,w)-b)\n",
    "    \n",
    "        # record weight and cost\n",
    "        weight_history. append(w)\n",
    "        cost_history.append(g(w))\n",
    "    \n",
    "    return weight_history, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84dc21b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m*\u001b[39mx\n\u001b[0;32m      6\u001b[0m g \u001b[38;5;241m=\u001b[39m grad(x2(\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[43mg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\autograd\\wrap_util.py:20\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(args[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m argnum)\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unary_operator(unary_f, x, \u001b[38;5;241m*\u001b[39mnary_op_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnary_op_kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\autograd\\differential_operators.py:25\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(fun, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;129m@unary_to_nary\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad\u001b[39m(fun, x):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    Returns a function which computes the gradient of `fun` with respect to\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    positional argument number `argnum`. The returned function takes the same\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    arguments as `fun`, but returns the gradient instead. The function `fun`\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    should be scalar-valued. The gradient has the same type as the argument.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     vjp, ans \u001b[38;5;241m=\u001b[39m \u001b[43m_make_vjp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vspace(ans)\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrad only applies to real scalar-output functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry jacobian, elementwise_grad or holomorphic_grad.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\autograd\\core.py:10\u001b[0m, in \u001b[0;36mmake_vjp\u001b[1;34m(fun, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_vjp\u001b[39m(fun, x):\n\u001b[0;32m      9\u001b[0m     start_node \u001b[38;5;241m=\u001b[39m VJPNode\u001b[38;5;241m.\u001b[39mnew_root()\n\u001b[1;32m---> 10\u001b[0m     end_value, end_node \u001b[38;5;241m=\u001b[39m  \u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvjp\u001b[39m(g): \u001b[38;5;28;01mreturn\u001b[39;00m vspace(x)\u001b[38;5;241m.\u001b[39mzeros()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\autograd\\tracer.py:10\u001b[0m, in \u001b[0;36mtrace\u001b[1;34m(start_node, fun, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace_stack\u001b[38;5;241m.\u001b[39mnew_trace() \u001b[38;5;28;01mas\u001b[39;00m t:\n\u001b[0;32m      9\u001b[0m     start_box \u001b[38;5;241m=\u001b[39m new_box(x, t, start_node)\n\u001b[1;32m---> 10\u001b[0m     end_box \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_box\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isbox(end_box) \u001b[38;5;129;01mand\u001b[39;00m end_box\u001b[38;5;241m.\u001b[39m_trace \u001b[38;5;241m==\u001b[39m start_box\u001b[38;5;241m.\u001b[39m_trace:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m end_box\u001b[38;5;241m.\u001b[39m_value, end_box\u001b[38;5;241m.\u001b[39m_node\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\autograd\\wrap_util.py:15\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f.<locals>.unary_f\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     subargs \u001b[38;5;241m=\u001b[39m subvals(args, \u001b[38;5;28mzip\u001b[39m(argnum, x))\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39msubargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "import autograd.numpy.random as npr\n",
    "\n",
    "def x2(x):\n",
    "    return x*x\n",
    "\n",
    "g = grad(x2(2))\n",
    "\n",
    "print (g(npr.randn(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4e79e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: \n",
      " [ 1.16136957e-03 -3.58870504e-01  4.81462918e+00  1.08097338e-02\n",
      "  3.33528185e-04 -1.06304081e-01 -3.06515432e-04 -4.67518265e-05\n",
      " -2.98901620e-02  4.22077985e-03]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This example shows how to define the gradient of your own functions.\n",
    "This can be useful for speed, numerical stability, or in cases where\n",
    "your code depends on external library calls.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "from autograd import grad\n",
    "from autograd.extend import primitive, defvjp\n",
    "from autograd.test_util import check_grads\n",
    "\n",
    "\n",
    "# @primitive tells Autograd not to look inside this function, but instead\n",
    "# to treat it as a black box, whose gradient might be specified later.\n",
    "# Functions with this decorator can contain anything that Python knows\n",
    "# how to execute, and you can do things like in-place operations on arrays.\n",
    "@primitive\n",
    "def logsumexp(x):\n",
    "    \"\"\"Numerically stable log(sum(exp(x))), also defined in scipy.special\"\"\"\n",
    "    max_x = np.max(x)\n",
    "    return max_x + np.log(np.sum(np.exp(x - max_x)))\n",
    "\n",
    "# Next, we write a function that specifies the gradient with a closure.\n",
    "# The reason for the closure is so that the gradient can depend\n",
    "# on both the input to the original function (x), and the output of the\n",
    "# original function (ans).\n",
    "\n",
    "def logsumexp_vjp(ans, x):\n",
    "    # If you want to be able to take higher-order derivatives, then all the\n",
    "    # code inside this function must be itself differentiable by Autograd.\n",
    "    # This closure multiplies g with the Jacobian of logsumexp (d_ans/d_x).\n",
    "    # Because Autograd uses reverse-mode differentiation, g contains\n",
    "    # the gradient of the objective w.r.t. ans, the output of logsumexp.\n",
    "    # This returned VJP function doesn't close over `x`, so Python can\n",
    "    # garbage-collect `x` if there are no references to it elsewhere.\n",
    "    x_shape = x.shape\n",
    "    return lambda g: np.full(x_shape, g) * np.exp(x - np.full(x_shape, ans))\n",
    "\n",
    "# Now we tell Autograd that logsumexmp has a gradient-making function.\n",
    "defvjp(logsumexp, logsumexp_vjp)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Now we can use logsumexp() inside a larger function that we want\n",
    "    # to differentiate.\n",
    "    def example_func(y):\n",
    "        z = y**2\n",
    "        lse = logsumexp(z)\n",
    "        return np.sum(lse)\n",
    "\n",
    "    grad_of_example = grad(example_func)\n",
    "    print(\"Gradient: \\n\", grad_of_example(npr.randn(10)))\n",
    "\n",
    "    # Check the gradients numerically, just to be safe.\n",
    "    check_grads(example_func, modes=['rev'])(npr.randn(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931a0bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
